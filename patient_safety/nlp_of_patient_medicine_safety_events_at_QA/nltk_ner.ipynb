{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with loading all necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import pyodbc\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\evansme\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\evansme\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\evansme\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\evansme\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\evansme\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\evansme\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.EntityRecognizer at 0x2a2f0420>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.set_proxy('https://evansme:Orange23@qahwsvip.pht-master.xports.nhs.uk:8080')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "from wordcloud import WordCloud\n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm',tagger=False, parser=False, matcher=False)\n",
    "nlp.entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inc_locactual</th>\n",
       "      <th>inc_unit</th>\n",
       "      <th>inc_loctype</th>\n",
       "      <th>inc_result</th>\n",
       "      <th>inc_severity</th>\n",
       "      <th>inc_notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E6/7</td>\n",
       "      <td>MEDMED</td>\n",
       "      <td>WARD</td>\n",
       "      <td>NOHARM</td>\n",
       "      <td>NONE</td>\n",
       "      <td>on drug rounds found Amlodipine not given for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C5</td>\n",
       "      <td>MEDMED</td>\n",
       "      <td>WARD</td>\n",
       "      <td>HARM</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HAEMOPHILIA PATIENT\\r\\n\\r\\nPATIENT NOT ADMINIS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EMERG</td>\n",
       "      <td>MEDUR</td>\n",
       "      <td>AE</td>\n",
       "      <td>NOHARM</td>\n",
       "      <td>NONE</td>\n",
       "      <td>Resus controlled order booked went missing, ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C7</td>\n",
       "      <td>MEDMED</td>\n",
       "      <td>WARD</td>\n",
       "      <td>NOHARM</td>\n",
       "      <td>NONE</td>\n",
       "      <td>MIDAZOLAM SIGNED OUT OF CD BOOK FOR PATIENT . ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D5</td>\n",
       "      <td>SURMHN</td>\n",
       "      <td>WARD</td>\n",
       "      <td>NOHARM</td>\n",
       "      <td>NONE</td>\n",
       "      <td>I was giving the oral medication(CD-oxycodone)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  inc_locactual inc_unit inc_loctype inc_result inc_severity  \\\n",
       "0          E6/7   MEDMED        WARD     NOHARM         NONE   \n",
       "1            C5   MEDMED        WARD       HARM          LOW   \n",
       "2         EMERG    MEDUR          AE     NOHARM         NONE   \n",
       "3            C7   MEDMED        WARD     NOHARM         NONE   \n",
       "4            D5   SURMHN        WARD     NOHARM         NONE   \n",
       "\n",
       "                                           inc_notes  \n",
       "0  on drug rounds found Amlodipine not given for ...  \n",
       "1  HAEMOPHILIA PATIENT\\r\\n\\r\\nPATIENT NOT ADMINIS...  \n",
       "2  Resus controlled order booked went missing, ha...  \n",
       "3  MIDAZOLAM SIGNED OUT OF CD BOOK FOR PATIENT . ...  \n",
       "4  I was giving the oral medication(CD-oxycodone)...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_conn = pyodbc.connect('DRIVER={SQL Server};'\n",
    "                            'SERVER=L_AAGDATIX;'\n",
    "                            'DATABASE=DatixCRM;'\n",
    "                            'Trusted_Connection=yes') \n",
    "query = \"set transaction isolation level read uncommitted select inc_locactual,inc_unit,inc_loctype,inc_result,inc_severity,inc_notes from DatixCRM.dbo.incidents_main where inc_type='PAT' and inc_category='MEDIC' and inc_organisation='QA'\"\n",
    "df = pd.read_sql(query, sql_conn)\n",
    "df= df[df.index < 100]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "medicine_mask = np.array(Image.open(\"bottle-white-vector-medicine.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_corpus = []\n",
    "tagged_tokesn = []\n",
    "token_trees = []\n",
    "label_value_dict = {}\n",
    "spacy_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "Person_list = []\n",
    "for details in df.inc_notes:                # Iterate over the files \n",
    "\n",
    "    #contents = details.lower() # lowercase contents\n",
    "\n",
    "    #tokens = [w for w in word_tokenize(details)\n",
    "    #     if w.isalnum()]     # Extract tokens\n",
    "    \n",
    "    #no_stops = [t for t in tokens\n",
    "    #       if t not in stopwords.words('english')]\n",
    "    \n",
    "    # Instantiate the WordNetLemmatizer\n",
    "    #wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Lemmatize all tokens into a new list: lemmatized\n",
    "    #lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "    #print(details)\n",
    "    # Tokenize the article into sentences: sentences\n",
    "    sentences = sent_tokenize(details)\n",
    "    #print(sentences)\n",
    "    # Tokenize each sentence into words: token_sentences\n",
    "    token_sentences = [word_tokenize(sent) for sent in sentences]\n",
    "    #print(token_sentences)\n",
    "    # Tag each tokenized sentence into parts of speech: pos_sentences\n",
    "    pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences] \n",
    "    #print(pos_sentences)\n",
    "    # Create the named entity chunks: chunked_sentences\n",
    "    chunked_sentences = nltk.ne_chunk_sents(pos_sentences)\n",
    "\n",
    "    # Create the defaultdict: ner_categories\n",
    "    ner_categories = defaultdict(int)\n",
    "\n",
    "    # Create the nested for loop\n",
    "    for sent in chunked_sentences:\n",
    "        for chunk in sent:\n",
    "            if hasattr(chunk, 'label'):\n",
    "                ner_categories[chunk.label()] += 1\n",
    "                if chunk.label()=='PERSON': \n",
    "                    for leaf in chunk.leaves():\n",
    "                        if 'NN' in leaf[1]:\n",
    "                            if len(leaf[0])>1 and leaf[0].isalpha():\n",
    "                                if leaf[0] not in Person_list: Person_list.append(leaf[0])\n",
    "            \n",
    "    # Append to list from the dictionary keys for the chart labels: labels\n",
    "    for key,val in ner_categories.items():\n",
    "        if key not in label_value_dict.keys():\n",
    "            label_value_dict[key] = 0\n",
    "        label_value_dict[key]+=val\n",
    "    \n",
    "    #lemmatized_corpus.append(lemmatized)\n",
    "    \n",
    "    #doc = nlp(details)\n",
    "    #print(doc.ents)\n",
    "    #print(doc.ents[0],doc.ents[0].label_)\n",
    "    \n",
    "    #for ent in doc.ents:\n",
    "    #    if ent.label_ not in spacy_dict.keys():\n",
    "    #        spacy_dict[ent.label_] = 0\n",
    "    #    spacy_dict[ent.label_]+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Amlodipine', 'Resus', 'Sister', 'Patient', 'Patches', 'Gastro', 'Naseptin', 'Procedure', 'Urology', 'Advagraf', 'Infusion', 'Location', 'Cd', 'Pharmacy', 'None', 'Insulin', 'Pharmacist', 'Warfarin', 'Oxynorm', 'Solution', 'Nurse', 'Handbag', 'Clinic', 'Parkinson', 'Junior', 'Rifater', 'Humulin', 'Stock', 'Dextrose', 'Renal', 'Penicillin', 'Lyclear', 'Dermal', 'Cream', 'Cottrell', 'Ward', 'Mesalazine', 'Mum', 'Fluids']\n"
     ]
    }
   ],
   "source": [
    "print(Person_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amlodipine\n",
      "0    False\n",
      "Name: inc_notes, dtype: bool\n",
      "Resus\n",
      "2    False\n",
      "Name: inc_notes, dtype: bool\n",
      "Sister\n",
      "2    False\n",
      "Name: inc_notes, dtype: bool\n",
      "Patient\n",
      "7      True\n",
      "8      True\n",
      "9     False\n",
      "18     True\n",
      "20     True\n",
      "26     True\n",
      "27     True\n",
      "31    False\n",
      "33    False\n",
      "35    False\n",
      "41    False\n",
      "43     True\n",
      "44     True\n",
      "46     True\n",
      "49     True\n",
      "54     True\n",
      "60     True\n",
      "61    False\n",
      "63     True\n",
      "65    False\n",
      "66     True\n",
      "72     True\n",
      "73     True\n",
      "75     True\n",
      "77    False\n",
      "78    False\n",
      "80     True\n",
      "84    False\n",
      "85    False\n",
      "87    False\n",
      "88     True\n",
      "89     True\n",
      "91    False\n",
      "92     True\n",
      "94    False\n",
      "Name: inc_notes, dtype: bool\n",
      "Gastro\n",
      "10    False\n",
      "Name: inc_notes, dtype: bool\n",
      "Naseptin\n",
      "15    True\n",
      "Name: inc_notes, dtype: bool\n",
      "Urology\n",
      "18    False\n",
      "Name: inc_notes, dtype: bool\n",
      "Advagraf\n",
      "20    False\n",
      "Name: inc_notes, dtype: bool\n",
      "Infusion\n",
      "28    False\n",
      "Name: inc_notes, dtype: bool\n",
      "Location\n",
      "32    False\n",
      "Name: inc_notes, dtype: bool\n",
      "Cd\n",
      "32    False\n",
      "Name: inc_notes, dtype: bool\n",
      "Pharmacy\n",
      "32     True\n",
      "53     True\n",
      "63    False\n",
      "Name: inc_notes, dtype: bool\n",
      "Insulin\n",
      "34    True\n",
      "Name: inc_notes, dtype: bool\n",
      "Warfarin\n",
      "35     True\n",
      "44    False\n",
      "Name: inc_notes, dtype: bool\n",
      "Solution\n",
      "40    False\n",
      "Name: inc_notes, dtype: bool\n",
      "Nurse\n",
      "7      True\n",
      "21    False\n",
      "43    False\n",
      "54    False\n",
      "79     True\n",
      "84    False\n",
      "Name: inc_notes, dtype: bool\n",
      "Clinic\n",
      "44    False\n",
      "Name: inc_notes, dtype: bool\n",
      "Parkinson\n",
      "53    False\n",
      "Name: inc_notes, dtype: bool\n",
      "Junior\n",
      "54    False\n",
      "Name: inc_notes, dtype: bool\n",
      "Rifater\n",
      "60    True\n",
      "Name: inc_notes, dtype: bool\n",
      "Stock\n",
      "70    True\n",
      "Name: inc_notes, dtype: bool\n",
      "Renal\n",
      "75    False\n",
      "90     True\n",
      "Name: inc_notes, dtype: bool\n",
      "Lyclear\n",
      "89    False\n",
      "Name: inc_notes, dtype: bool\n",
      "Dermal\n",
      "89    False\n",
      "Name: inc_notes, dtype: bool\n",
      "Cream\n",
      "89    False\n",
      "Name: inc_notes, dtype: bool\n",
      "Cottrell\n",
      "90    False\n",
      "Name: inc_notes, dtype: bool\n",
      "Ward\n",
      "40    False\n",
      "72     True\n",
      "90     True\n",
      "Name: inc_notes, dtype: bool\n",
      "Mum\n",
      "97    False\n",
      "Name: inc_notes, dtype: bool\n",
      "Fluids\n",
      "98    True\n",
      "Name: inc_notes, dtype: bool\n",
      "['Amlodipine', 'Resus', 'Sister', 'Patches', 'Gastro', 'Procedure', 'Urology', 'Advagraf', 'Infusion', 'Location', 'Cd', 'None', 'Pharmacist', 'Oxynorm', 'Solution', 'Handbag', 'Clinic', 'Parkinson', 'Junior', 'Humulin', 'Dextrose', 'Penicillin', 'Lyclear', 'Dermal', 'Cream', 'Cottrell', 'Mesalazine', 'Mum']\n"
     ]
    }
   ],
   "source": [
    "for Person in Person_list:\n",
    "    df_19 = df[df['inc_notes'].str.contains(Person)]\n",
    "    print(Person)\n",
    "    print(df_19['inc_notes'].str.contains(Person.lower()))\n",
    "    if df_19['inc_notes'].str.contains(Person.lower()).any(): Person_list.remove(Person)\n",
    "print(Person_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Reported by B8 \\r\\nPatients drug chart sent to pharmacy to get nicotine patches. Patches came to ward with no drug chart']\n"
     ]
    }
   ],
   "source": [
    "df_19 = df[df['inc_notes'].str.contains('Patches')]\n",
    "print(df_19['inc_notes'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sister', 'Patches', 'Gastro', 'Naseptin', 'Procedure', 'Patient', 'Patient', 'Infusion', 'Location', 'None', 'Insulin', 'Patient', 'Warfarin', 'Oxynorm', 'Solution', 'Patient', 'Nurse', 'Patient', 'Patient', 'Parkinson', 'Nurse', 'Patient', 'Rifater', 'Patient', 'Humulin', 'Patient', 'Patient', 'Stock', 'Dextrose', 'Patient', 'Patient', 'Patient', 'Renal', 'Penicillin', 'Patient', 'Patient', 'Lyclear', 'Dermal', 'Cream', 'Cottrell', 'Renal', 'Ward', 'Mesalazine', 'Mum', 'Fluids']\n"
     ]
    }
   ],
   "source": [
    "if 'Amlodipine' in person_list: person_list.remove('Amlodipine')\n",
    "if 'Resus' in person_list: person_list.remove('Resus')\n",
    "if 'Patient' in person_list: person_list.remove('Patient')\n",
    "print(person_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Category of named entity in details field')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the pie chart\n",
    "plt.pie(label_value_dict.values(), labels=label_value_dict.keys(), autopct='%1.0f%%');\n",
    "plt.legend(loc=(1.04,0.5));\n",
    "plt.title(\"Category of named entity in details field\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-85-5053841a3d90>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0moperator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0msorted_named_entity_percentages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnamed_entity_percentages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moperator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitemgetter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbarh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msorted_named_entity_percentages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbarh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msorted_named_entity_percentages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msorted_named_entity_percentages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbarh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msorted_named_entity_percentages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msorted_named_entity_percentages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0msorted_named_entity_percentages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "named_entity_tot = sum(label_value_dict.values())\n",
    "named_entity_percentages = {}\n",
    "for key,val in label_value_dict.items():\n",
    "    named_entity_percentages[key] = 100*val/named_entity_tot\n",
    "import operator\n",
    "sorted_named_entity_percentages = sorted(named_entity_percentages.items(), key=operator.itemgetter(1), reverse=True)\n",
    "plt.barh(1,sorted_named_entity_percentages[0][1])\n",
    "plt.barh(1,sorted_named_entity_percentages[1][1],left=sorted_named_entity_percentages[0][1])\n",
    "plt.barh(1,sorted_named_entity_percentages[2][1],left=sorted_named_entity_percentages[0][1]+sorted_named_entity_percentages[1][1])\n",
    "plt.xlabel(\"%\")\n",
    "plt.title(\"Category of named entity in details field\")\n",
    "fig = plt.gca()\n",
    "fig.get_yaxis().set_visible(False)\n",
    "fig.text(0+5,1,sorted_named_entity_percentages[0][0],color='white');\n",
    "fig.text(sorted_named_entity_percentages[0][1]+5,1,sorted_named_entity_percentages[1][0],color='white');\n",
    "fig.text(sorted_named_entity_percentages[0][1]+sorted_named_entity_percentages[1][1]+5,1,sorted_named_entity_percentages[2][0],color='white');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in spacy_dict.keys():\n",
    "    print(key + ': ',spacy.explain(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "named_entity_tot = sum(spacy_dict.values())\n",
    "named_entity_percentages = {}\n",
    "spacy_dict['other number'] = spacy_dict['CARDINAL']+spacy_dict['QUANTITY']\n",
    "del spacy_dict['CARDINAL']\n",
    "del spacy_dict['QUANTITY']\n",
    "spacy_dict['location'] = spacy_dict['GPE']+spacy_dict['FAC']+spacy_dict['LOC']\n",
    "del spacy_dict['GPE']\n",
    "del spacy_dict['FAC']\n",
    "del spacy_dict['LOC']\n",
    "for key,val in spacy_dict.items():\n",
    "    named_entity_percentages[key] = 100*val/named_entity_tot\n",
    "import operator\n",
    "sorted_named_entity_percentages = sorted(named_entity_percentages.items(), key=operator.itemgetter(1), reverse=True)\n",
    "named_entity_percentage = [sorted_named_entity_percentages[i][1] for i in range(len(sorted_named_entity_percentages))]\n",
    "named_entity_label = [sorted_named_entity_percentages[i][0].lower() for i in range(len(sorted_named_entity_percentages))]\n",
    "named_entity_label = [ne.replace('cardinal','other number') for ne in named_entity_label]\n",
    "named_entity_label = [ne.replace('org','organisation') for ne in named_entity_label]\n",
    "named_entity_label = [ne.replace('gpe','town,city..') for ne in named_entity_label]\n",
    "named_entity_label = [ne.replace('ordinal','\"1st\",\"2nd\"..') for ne in named_entity_label]\n",
    "named_entity_label = [ne.replace('norp','nationality/religion/political group') for ne in named_entity_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the spacy pie chart\n",
    "plt.pie(named_entity_percentage, labels=named_entity_label);\n",
    "plt.legend(loc=(1.75,0.1));\n",
    "plt.title(\"Category of named entity in details field for medicine patient safety events at QA\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(lemmatized_corpus) # Build the dictionary\n",
    "\n",
    "\n",
    "# Convert to vector corpus\n",
    "\n",
    "vectors = [dictionary.doc2bow(text) for text in lemmatized_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the word cloud\n",
    "\n",
    "wc = WordCloud(\n",
    "    background_color=\"white\",\n",
    "    max_words=20,\n",
    "    width = 1024,\n",
    "    height = 720,\n",
    "    mask=medicine_mask,\n",
    "    stopwords=stopwords.words(\"english\")\n",
    ")\n",
    "\n",
    "# Generate the cloud\n",
    "words_dict = dict(dictionary)\n",
    "words = \" \".join(w for w in words_dict.values())\n",
    "print(words)\n",
    "wc.generate(words)\n",
    "\n",
    "print(wc.words_)\n",
    "\n",
    "# Display the generated image:\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\" \".join(w for w in df['inc_notes'].values))\n",
    "trainer = nltk.tokenize.punkt.PunktTrainer(words)\n",
    "trainer.INCLUDE_ALL_COLLOCS = True \n",
    "trainer.INCLUDE_ABBREV_COLLOCS = True\n",
    "params = trainer.get_params()\n",
    "trainer.train(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build TF-IDF model\n",
    "\n",
    "tfidf = TfidfModel(vectors)\n",
    "\n",
    "# Get TF-IDF weights\n",
    "\n",
    "weights = tfidf[vectors[0]]\n",
    "\n",
    "# Get terms from the dictionary and pair with weights\n",
    "\n",
    "weights = [(dictionary[pair[0]], pair[1]) for pair in weights]\n",
    "\n",
    "# Initialize the word cloud\n",
    "\n",
    "wc = WordCloud(\n",
    "    background_color=\"white\",\n",
    "    max_words=100,\n",
    "    width = 1024,\n",
    "    height = 720,\n",
    "    mask=medicine_mask,\n",
    "    stopwords=stopwords.words(\"english\")\n",
    ")\n",
    "\n",
    "# Generate the cloud\n",
    "weights_dict = dict(weights)\n",
    "wc.generate_from_frequencies(weights_dict)\n",
    "\n",
    "# Display the generated image:\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
